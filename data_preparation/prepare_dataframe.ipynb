{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import smplx\n",
    "import cv2\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(raw_name, raw_index, dataset):\n",
    "    '''\n",
    "    raw_name: image_name, e.g., ag_trainset_renderpeople_bfh_archviz_5_10_cam02_00001.png\n",
    "    raw_index: index of person in the image (dataframe[\"min_occ_idx\"])\n",
    "    dataset: for example, train_0\n",
    "    '''\n",
    "\n",
    "    # generate img path\n",
    "    img_name = raw_name.replace('.png','_1280x720.png')\n",
    "    img_name_ele = img_name.split(\"_\")\n",
    "    img_path = \"./{}/{}\".format(dataset, img_name)\n",
    "\n",
    "\n",
    "    img_name_ele[-2] = \"0\"+img_name_ele[-2]\n",
    "    if (raw_index+1<10):\n",
    "        img_name_ele.insert(-1,\"0000{}\".format(raw_index+1)) \n",
    "    else:\n",
    "        img_name_ele.insert(-1,\"000{}\".format(raw_index+1)) \n",
    "    \n",
    "    # generate target path\n",
    "    tgt_path = \"_\".join(img_name_ele) # for example, ag_trainset_renderpeople_bfh_archviz_5_10_cam02_000001_00001_1280x720.png\n",
    "    tgt_path = \"./dataset/{}/{}_{}\".format(dataset.split(\"_\")[0], dataset, tgt_path)\n",
    "\n",
    "    \n",
    "    # generate mask path\n",
    "    mask_folder = \"_\".join(img_name_ele[:5])\n",
    "\n",
    "    if (img_name_ele[-4].startswith(\"cam\")):\n",
    "        img_name_ele.insert(-4,\"mask\")\n",
    "    else:\n",
    "        img_name_ele.insert(-3,\"mask\")\n",
    "\n",
    "\n",
    "    mask_name = \"_\".join(img_name_ele) # for example, ag_trainset_renderpeople_bfh_archviz_5_10_mask_cam02_000001_00001_1280x720.png\n",
    "    mask_path = \"./train_masks_1280x720/train/{}/{}\".format(mask_folder,mask_name) \n",
    "\n",
    "   \n",
    "\n",
    "    return img_path, tgt_path, mask_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_image(img_path, tgt_path, mask_path):\n",
    "    try:\n",
    "        # get mask image of selected person\n",
    "        img = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path, 0) # for foreground (person)\n",
    "        masked_img = cv2.bitwise_and(img, img, mask=mask)\n",
    "        new_mask = np.logical_not(mask) # for background => we want white background eventually\n",
    "        masked_img[new_mask]=255 # new_mask contains boolean entries and therefore can be used in this way\n",
    "        masked_img = cv2.cvtColor(masked_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # crop image from the mask\n",
    "        c = np.nonzero(mask)\n",
    "        x_min = int(min(c[1]))\n",
    "        x_max = int(max(c[1]))\n",
    "        y_min = int(min(c[0]))\n",
    "        y_max = int(max(c[0]))\n",
    "        cropped_img = masked_img[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        w = x_max - x_min\n",
    "        h = y_max - y_min\n",
    "\n",
    "        # scale the cropped image\n",
    "        scale = 200/max(w, h)\n",
    "        resized_w = int(scale*w)\n",
    "        resized_h = int(scale*h)\n",
    "        resized_cropped_img = cv2.resize(cropped_img, (resized_w, resized_h))\n",
    "\n",
    "        # generate final result (256*256 white background image)\n",
    "        final_result = np.zeros((256,256,3))\n",
    "        final_c_x = 128\n",
    "        final_c_y = 128\n",
    "        final_result += 255\n",
    "\n",
    "        final_result[int(final_c_y-resized_h/2):int(final_c_y+resized_h/2),int(final_c_x-resized_w/2):int(final_c_x+resized_w/2)] = resized_cropped_img\n",
    "        final_result = final_result.astype(int) # necessary\n",
    "\n",
    "        plt.imshow(final_result)\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig(\"{}\".format(tgt_path))\n",
    "    except:\n",
    "        print (img_path, tgt_path, mask_path)\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = []\n",
    "\n",
    "# read data for each of the 10 training group\n",
    "for i in range(10):\n",
    "    df = pd.read_pickle(\"./SMPLX/train_{}_withjv.pkl\".format(i))[[\"imgPath\", \"occlusion\", \"gt_path_smplx\"]]\n",
    "    df[\"dataset\"] = \"train_{}\".format(i)\n",
    "    df[\"indices\"] = df.apply(lambda x: list(range(len(x[\"occlusion\"]))), axis=1)\n",
    "    df = df.explode(\"indices\")\n",
    "    df[\"smplx_path\"] = df.apply(lambda x: x[\"gt_path_smplx\"][x[\"indices\"]], axis=1)\n",
    "    df[\"occlusions\"] = df.apply(lambda x: x[\"occlusion\"][x[\"indices\"]], axis=1)\n",
    "    paths = df.apply(lambda x: get_paths(x[\"imgPath\"], x[\"indices\"], x[\"dataset\"]), axis=1)\n",
    "    df[\"src_img_path\"] = paths.apply(lambda x: x[0])\n",
    "    df[\"tgt_img_path\"] = paths.apply(lambda x: x[1])\n",
    "    df[\"mask_path\"] = paths.apply(lambda x: x[2])\n",
    "\n",
    "    train_df.append(df[[\"dataset\", \"src_img_path\", \"mask_path\", \"tgt_img_path\", \"indices\", \"occlusions\"]])\n",
    "\n",
    "train_df = pd.concat(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9655, 6)\n"
     ]
    }
   ],
   "source": [
    "# select threshold of occlusions for training images\n",
    "final_train_df = train_df[(train_df[\"occlusions\"]>=0) & (train_df[\"occlusions\"]<0.3)]\n",
    "print (final_train_df.shape)\n",
    "final_train_df.to_csv(\"train_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Developing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = final_train_df.iloc[:100]\n",
    "dev_df.to_csv(\"dev_dataframe.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Validation Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_pickle(\"./SMPLX/valid_withjv.pkl\".format(i))[[\"imgPath\", \"occlusion\", \"gt_path_smplx\"]]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06d902bdc97db2dad3edace23fd55d98f4eff955da063389037d7edaf0eaf8c4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('p3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
